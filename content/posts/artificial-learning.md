+++
author = "Jovie"
title = "The Epidemic of Artificial Learning"
date = "2024-01-02"
tags = [
    "Learning", "AI", "Software Development"
]
+++

I am a college student studying Software Development. When I tell people that, the first question they ask me is -- without fail -- "Is AI going to impact software development? I heard it's really good at coding."

Now, it's a totally valid question, especially since most non-techie folks don't really know how large language models work. And, as a student, I am obviously not the authority on whether or not "AI" is going to take my job. I'm just a student, right?

But I do have a perspective that maybe some industry people don't have. I am in class learning with people who exclusively use large language models as a form of documentation. We're not learning anything crazy in this program; basic React, Next.js, C#, Java, etc. But the proportion of people who go straight to an LLM to answer their question or fix their code is above 70%. I would say that 90% have never heard of the `man` function on Unix, and the same amount don't know how to effectively read, or even find, documentation. Now, I'll give them a bit of a break; current-day Javascript framework documentation, even React, can be pretty horrible at times. Dead links and outdated information can quickly become a chore to sort through. However, the people I go to school with wouldn't know this since they don't look at documentation.

If you've been in the software industry for longer than five years, this might come as a shock to you. How do people who don't "read the... friendly manual" do... anything? Spoiler alert; they don't. And again, I'm probably being a little harsh, but hear me out.

My software development program transitioned last year to being a strictly project-only program. This means we don't have any tests, and the only things we are graded on are software projects. I could see how they thought this was a good idea, but the fact of the matter is that the projects they're giving us are just too darn simple. And yes, because they are simple, ChatGPT can _easily_ help out with these projects. However, since people in this program are relying heavily on some digested knowledge slopped together from who knows how many sources, they don't _learn_ anything.

I have a vivid memory of one of my instructors asking me to teach the class since he was busy helping people set up their React Native environments. Once I got up to the front of the class I plugged my laptop into the HDMI and displayed my code for a password-generator user interface. After getting blank stares from my peers while I was talking about my code, I realized maybe I was going too fast, or maybe they didn't understand something I said. I asked if everyone understood functions (keep in mind, this is the second year of this software dev program), and maybe two out of 30 people raised their hands. I couldn't believe it.

And this is the problem of artificial learning. These people had been successfully handing in project after project for a year at this point without understanding any of what they submitted. My school actually _encourages_ the use of language models to assist you in your learning. But I argue that this creates bad programmers and keeps people at the very beginning of the dunning-kruger curve indefinitely. To have something generate the solution to your problem without you putting in the effort to understand the problem in the first place keeps you in the dark.

None of this is to say that if you use an LLM to code then you're a bad coder. However, the process of learning is NOT easy. To say that one is learning when they copy and paste Gemini-generated code into their VSCode and get it running correctly by asking Gemini questions on how to get it running correctly is just... false. Learning SHOULD BE DIFFICULT. Reading through documentation is not easy, but you can only become effective at combing through documentation by doing it.

Learning to read code is another skill that comes only by doing it frequently and intentionally. Too often I see people shamelessly copy a function into an LLM and ask what it does. The LLM is not going to give you a good answer; it doesn't know anything about the codebase! And even if it does, such as if you have an editor with a codebase-ingesting LLM plugin, you're _not learning._ You're simply asking a neural network what it thinks the code does. Being able to effectively read code is invaluable if you want to be a software developer who doe anything valuable.

And I think using a language model to give you all the answers takes away the _fun._ Why ask a probabilistic model what something does when you can look at it and SEE what it does? Why get a half-baked answer that leads you astray when you can get the correct, factual answer and learn something along the way?

Language models are creating a generation of apathetic software developers who don't care about their code. I see this as potentially disastrous for the future of software in general. Do we want slop code eventually being injected into the Linux kernel? It is a good idea to rely on a model to generate safe C code? Is a future where websites are all homogenized a good future? I say no. This is why I am against artificial learning.
